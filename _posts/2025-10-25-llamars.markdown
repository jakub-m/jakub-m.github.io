---
layout: post
title: llama2 in Rust, with Metal and Rayon
date:  2025-10-25  00:00:00 -0000
permalink: llama2-rs
---

**For the up date version of the post see my [llama2-rs](https://github.com/jakub-m/llama2-rs) on GitHub.**

---

# llama2-rs

This is a rewrite of [llama2.c][llama2_c] to Rust and Apple M2 Metal GPU, with
a bit of [Rayon][rayon]. This code is just for learning purposes, it is not
maintained. Below are the learnings.

## Prerequisites

You need `tokenizer.bin` from the original llama2.c repo, also you need the
[TinyStories][TinyStories] model, or a full llama2 7B model. See
[llama2.c][llama2_c] on how to download and convert the model.

[rayon]: https://github.com/rayon-rs/rayon
[TinyStories]: https://huggingface.co/datasets/roneneldan/TinyStories
[llama2_c]: https://github.com/karpathy/llama2.c



# Diagrams

Here are some diagrams that I draw along the implementation to understand what
the code actually does. Those diagrams later helped me to figure which matrix
multiplications can be run independently.


## High level

[![](https://mermaid.ink/img/pako:eNp1U21v0zAQ_iuWJ6RWtFXXNG0X6CREBx82KmDAJDCqvPjSRHPszHHoyrT_zsXuOu-FSJHufPfcy-PHtzTVAmhCM6k3ac6NJd8WTDHFhVhlSq3qTbGWTafD6GtGu10fMFAXouEyPLYWlC20wqN39zZ5e2mOyZLkwEXtnU7FDZcSZBdxTEF5CUIUao2wk3vbZ2bakEJVjSVWX4Fy6cFEjH5YLslmQM43xcez7x5zcdgjFyP8I_wx2YEk34JZjRCxhBtLnDsYDIJY1FYrFJc-6CN6Xdh6VfOykpgioCXKYOLXT-dEaVNih1Tutjpzyd4-9wjvLDzMVbxuwGxXV7DFIl9a26ecwpakPM1ht8I1Fr5yAFPWq7bTClkKGvuYrqDzCw_15xNGf3fbOwvoJP3-MQnxQazfR9juBn3PVCsFaXtjjCIwvOK26iv8PDN4IRYMElWT9pCpurlcG17lhNEzT10LCBu7SfarByz4EXENv4xz9yp6VoPRH1w28IipP27aEBTocD5n9EIzOp8_2yf0XdsHWYUS8xM9egcvYJ8mINFK7DhD6z-8PX1grtROqHvFurWdUh-i0V6zHvKiSr0Y2iFIbbcSaqZSyet6ARkRvM5BkKyQMjnIsqxXW4PvKzkYDoc7u9_mcGP4NiExid8E6IxbskvaFMLmCYmqm_uEh5to8_wItEfXphA0yXB56NESTMlbn94yRVA2NocSGE3QFJDxRtpWQ3eIq7j6qXVJE2saRBrdrPN9naYS3MKi4Ci_cn9qkHEw73WjLE0O46ErQpNbekOT0SwejCeT4WQ8jqPDUdyjW5qMjwbxZDSezeI4PhrORnc9-tf1HA5m0_ERftNoOh1F0d0_GXvCQQ?type=png)](https://mermaid.live/edit#pako:eNp1U21v0zAQ_iuWJ6RWtFXXNG0X6CREBx82KmDAJDCqvPjSRHPszHHoyrT_zsXuOu-FSJHufPfcy-PHtzTVAmhCM6k3ac6NJd8WTDHFhVhlSq3qTbGWTafD6GtGu10fMFAXouEyPLYWlC20wqN39zZ5e2mOyZLkwEXtnU7FDZcSZBdxTEF5CUIUao2wk3vbZ2bakEJVjSVWX4Fy6cFEjH5YLslmQM43xcez7x5zcdgjFyP8I_wx2YEk34JZjRCxhBtLnDsYDIJY1FYrFJc-6CN6Xdh6VfOykpgioCXKYOLXT-dEaVNih1Tutjpzyd4-9wjvLDzMVbxuwGxXV7DFIl9a26ecwpakPM1ht8I1Fr5yAFPWq7bTClkKGvuYrqDzCw_15xNGf3fbOwvoJP3-MQnxQazfR9juBn3PVCsFaXtjjCIwvOK26iv8PDN4IRYMElWT9pCpurlcG17lhNEzT10LCBu7SfarByz4EXENv4xz9yp6VoPRH1w28IipP27aEBTocD5n9EIzOp8_2yf0XdsHWYUS8xM9egcvYJ8mINFK7DhD6z-8PX1grtROqHvFurWdUh-i0V6zHvKiSr0Y2iFIbbcSaqZSyet6ARkRvM5BkKyQMjnIsqxXW4PvKzkYDoc7u9_mcGP4NiExid8E6IxbskvaFMLmCYmqm_uEh5to8_wItEfXphA0yXB56NESTMlbn94yRVA2NocSGE3QFJDxRtpWQ3eIq7j6qXVJE2saRBrdrPN9naYS3MKi4Ci_cn9qkHEw73WjLE0O46ErQpNbekOT0SwejCeT4WQ8jqPDUdyjW5qMjwbxZDSezeI4PhrORnc9-tf1HA5m0_ERftNoOh1F0d0_GXvCQQ)




##  Detailed

The detailed flow diagram based directly on the code:


[![](https://mermaid.ink/img/pako:eNqlV21v2zYQ_isEgWIxlqaOLGeO0TYL2nT7kCVF2iHYokCQLNoSJJG2XmylQf777khKoiUl2TAjkKi75-6eOx5f8kgXImB0TpeJ2C1CLyvI988OdzhLfRYEEV8dOLQZk_d-9u4jucvZxk0YJxUJovReSw8KEY8cOjJsydu3H0nlHjscHuCoqh2gmYQ6_M0bkpf-KvPWIfkiMsK8RUgS74FlqM3SnIssBWswv_nj2xV8dJyMJMr1isLdsWgVFgpJQHBElKS2uOwwlmEU58pXDP1hit_DKCfwt0SGW5Y9KIoEfoiQCWK2Dd8uKVmKfa0aqhr5KHP4zt0AidtNh_DztDclcAET-e5RB3fSfeoVaZmAb0D-qkqGEUFnKB1ej6SN9IicYoYB4EkWMDXMpNb2Qbx1JUPF7ZCsRT7SFGLMKO5nVJv0kwIbk3b8Eu24oa1sgKcq5BbDbv9L2K2XlAys5PvfJGu60ClzncDWTGD7UgLbJgFlI6PL1hdrBlN2B90svl6QA1XS-3rSVUNJTIOOh9A4cw02frafoVUZLyLBSci8oA4PlnfcRUkOiePbzaMfDCYaxyP0iz2-qQkg_uzs7L5WxLqsXTlk3az6C1zx3fC4sqTnprnzJFowKCKpfQSiwFVTI2Pdpa_h5CcWCgZYoRrXIMBPyyZfiIzlprtcLIvUq1qHWoAua50sfK3fNj1VU-vZgu-ammUYDovl50ACbuW3oMq_6-9m97A38wBbQEHAUPde5VtGk-5cgatHkPfv_GxgG5LSdunsuWnaGcagkBL0jmyszg6F-QWBWyk72EK7Amn2U29fazDI-WfkjJGwRg5fcu7qzfXVMwOxO8wUBq-eERDC8C3Z9US-Yuz3KRs4WRXXUnDtQtcs9IelMrP2mLy6IruoCMm3XfTb5Z9q0vfwxlzWOhA6NGwaIoyCgHG34YewHZ6At8f9PdMAD-yb6hBUDmQvdhLqsdPZoW5Aaw2StyR760X6E6Q_-R_0J3361hDDJgHLzCBUsx-q2X-GZL6LVkkpp0jN3n6akkCD2SvAkKr5bIiHvmvtU5JrXt5SoLMmqj1rhu1NAZnfWlr8bOn2K2cUZRlxLzF3EKtbSolQNsjqBbUpUWtlom5Yyqhe_CjFZ1-osuwn2YKafcMU6tuqDjYhHz4Qh15eX38lX65vyMX5p9_J5flfFzfKLfqP8FCMOOGsKkhUsMzDM8OhYKrvvfKwxfove1dbUIiyyKOAkUSItd6RMOn9e6wUqZjtbbZNa-9iqau492nsgvoK4tqqPr0Cqawder6EXHRgxZdiRi9F0cWz5eVrkeTYTvBqI0CltmLh-er-IMMlYhUViFQDje2idDtIjHmRcm3zjiXVKvSA2PySahWxpqCWlpeuE5waPZDCgOH_R3K-6CFdZVFA50svydkhTVmWevhNH3HBObQIWQqn-xyGAVt6ZQKns8OfwG7t8b-FSOm8yEqwzES5Chs_5TrwCvY58mBzTxtpBk3Dsk-i5AWdH09t6YTOH2lF59ZsemSfnIxPbHs6Obamh_SBzu3To-mJZc9m0-n0dDyzng7pDxlzfDT7xT6F32w8to_tif30D8UukHA?type=png)](https://mermaid.live/edit#pako:eNqlV21v2zYQ_isEgWIxlqaOLGeO0TYL2nT7kCVF2iHYokCQLNoSJJG2XmylQf777khKoiUl2TAjkKi75-6eOx5f8kgXImB0TpeJ2C1CLyvI988OdzhLfRYEEV8dOLQZk_d-9u4jucvZxk0YJxUJovReSw8KEY8cOjJsydu3H0nlHjscHuCoqh2gmYQ6_M0bkpf-KvPWIfkiMsK8RUgS74FlqM3SnIssBWswv_nj2xV8dJyMJMr1isLdsWgVFgpJQHBElKS2uOwwlmEU58pXDP1hit_DKCfwt0SGW5Y9KIoEfoiQCWK2Dd8uKVmKfa0aqhr5KHP4zt0AidtNh_DztDclcAET-e5RB3fSfeoVaZmAb0D-qkqGEUFnKB1ej6SN9IicYoYB4EkWMDXMpNb2Qbx1JUPF7ZCsRT7SFGLMKO5nVJv0kwIbk3b8Eu24oa1sgKcq5BbDbv9L2K2XlAys5PvfJGu60ClzncDWTGD7UgLbJgFlI6PL1hdrBlN2B90svl6QA1XS-3rSVUNJTIOOh9A4cw02frafoVUZLyLBSci8oA4PlnfcRUkOiePbzaMfDCYaxyP0iz2-qQkg_uzs7L5WxLqsXTlk3az6C1zx3fC4sqTnprnzJFowKCKpfQSiwFVTI2Pdpa_h5CcWCgZYoRrXIMBPyyZfiIzlprtcLIvUq1qHWoAua50sfK3fNj1VU-vZgu-ammUYDovl50ACbuW3oMq_6-9m97A38wBbQEHAUPde5VtGk-5cgatHkPfv_GxgG5LSdunsuWnaGcagkBL0jmyszg6F-QWBWyk72EK7Amn2U29fazDI-WfkjJGwRg5fcu7qzfXVMwOxO8wUBq-eERDC8C3Z9US-Yuz3KRs4WRXXUnDtQtcs9IelMrP2mLy6IruoCMm3XfTb5Z9q0vfwxlzWOhA6NGwaIoyCgHG34YewHZ6At8f9PdMAD-yb6hBUDmQvdhLqsdPZoW5Aaw2StyR760X6E6Q_-R_0J3361hDDJgHLzCBUsx-q2X-GZL6LVkkpp0jN3n6akkCD2SvAkKr5bIiHvmvtU5JrXt5SoLMmqj1rhu1NAZnfWlr8bOn2K2cUZRlxLzF3EKtbSolQNsjqBbUpUWtlom5Yyqhe_CjFZ1-osuwn2YKafcMU6tuqDjYhHz4Qh15eX38lX65vyMX5p9_J5flfFzfKLfqP8FCMOOGsKkhUsMzDM8OhYKrvvfKwxfove1dbUIiyyKOAkUSItd6RMOn9e6wUqZjtbbZNa-9iqau492nsgvoK4tqqPr0Cqawder6EXHRgxZdiRi9F0cWz5eVrkeTYTvBqI0CltmLh-er-IMMlYhUViFQDje2idDtIjHmRcm3zjiXVKvSA2PySahWxpqCWlpeuE5waPZDCgOH_R3K-6CFdZVFA50svydkhTVmWevhNH3HBObQIWQqn-xyGAVt6ZQKns8OfwG7t8b-FSOm8yEqwzES5Chs_5TrwCvY58mBzTxtpBk3Dsk-i5AWdH09t6YTOH2lF59ZsemSfnIxPbHs6Obamh_SBzu3To-mJZc9m0-n0dDyzng7pDxlzfDT7xT6F32w8to_tif30D8UukHA)

# Learnings

Rust:

- Using Rust type (like `TokenId` instead of `usize`) helps to understand the
  code.

- Use `assert!` a lot. When you have matrices or vectors that assume some
  sizing, add assertions to find the bugs early. Most of this code deals with
  buffers of different sizes, and the sizing is often the only thing that you
  can assume. An assertion firing was several times the only thing that hinted
  me that I confused dimensions.

- If you don't keep [Mmap](https://docs.rs/memmap2/latest/memmap2/) object
  alive, it will be dropped and accessing the pointed data will result in a
  segfault.

- Use `cargo objdump --bin llama2-rs -- -d -S ./target/debug/llama2-rs` to see
  the assembly output, e.g. to see if the code is vectorised.

- You can check if the generated code uses [SIMD and vectorization][vfma_rust]
  by inspecting disassembled code, and looking for [SIMD
  instructions][vfma_arm] (that [start with "f" like `fmul`][fmul]):

  ```bash
  make objdump-llama | egrep '^000| \tf' | grep llama2_rs -A1 
  
  0000000100005494 <_llama2_rs::main::h9ba2e6463cb6eab5>:
  100005710: 1e202008    	fcmp	s0, #0.0
  ```

[vfma_rust]: https://doc.rust-lang.org/core/arch/aarch64/fn.vfma_n_f32.html
[vfma_arm]: https://developer.arm.com/architectures/instruction-sets/intrinsics/vfma_n_f32
[fmul]: https://developer.arm.com/documentation/ddi0602/2025-06/SIMD-FP-Instructions/FMUL--by-element---Floating-point-multiply--by-element--


[Rayon][rayon]:

- Just slapping `rayon` and  [`par_iter`][par_iter] tremendously speeds up the
  code.

- Use `RAYON_NUM_THREADS=1` for sequential execution, good for debugging.

[par_iter]: https://docs.rs/rayon/latest/rayon/iter/index.html


Metal:

- Just slapping GPU at the problem (this problem at least) will not make it
  magically faster.

- I didn't see much difference in performance when using shared or private GPU
  memory buffers. Maybe it's because of specific access patterns of the
  program.


Float16:

- 65520 is already an infinity in f16.

- Apple M2 Metal does not support matrix multiplication on BFloat16:

  ```text
  MPSMatrixMultiplication.mm:3260: failed assertion `Input data type must be one
  of MPSDataTypeFloat32, MPSDataTypeFloat16, MPSDataTypeInt8, or
  MPSDataTypeInt16.'
  ```

- Also, you cannot multiply int and float in a single matmul:
  ```text
  MPSMatrixMultiplication.mm:3250: failed assertion `Mixed input matrix
  multiplication is only supported when A.dataType == C.dataType ==
  MPSDataTypeFloat32 and B.dataTyp e == MPSDataTypeFloat16.'
  ```

- When doing matmul on f16, you need the output to be f32, otherwise the output
  will corrupt. Fortunately, Metal supports matmul for f16 at input and f32 at output.

- Using f16 was _slower_ for TinyStories than f32.


Other:

- ChatGPT was _very useful_ in learning `lldb` commands.

- [mermaid](https://mermaid.live) is an absolutely fantastic tool for diagrams.

- Monitor if you memory does not start to swap , with `sysctl vm.swapusage` or
  Activity Monitor. If mem swaps, the computation will instantly become dog
  slow.

- Apple's Instruments is useful to see GPU utilization, flamegraphs, etc. You
  need [cargo instruments][cargo_instruments]. See [Makefile and
  instruments-tinystories there](https://github.com/jakub-m/llama2-rs/blob/047c73611a65fdbcf7f61cf1d019c557800280e9/Makefile) on how to use that. Mind that you
  need debug symbols in the binary, with `debug = true` in
  [Cargo.toml](https://github.com/jakub-m/llama2-rs/blob/047c73611a65fdbcf7f61cf1d019c557800280e9/Cargo.toml)

[cargo_instruments]: https://crates.io/crates/cargo-instruments


# Benchmarking Rayon

I didn't run very rigorous benchmarks, rather run inference on TinyStories 42M
model, and also on llama2 7B model.

- 34 tok/s - TinyStories, no parallelization, no rayon, sequential as it can
  be. [commit](https://github.com/jakub-m/llama2-rs/commit/44fce5a)

- 27 tok/s - using naively [par_bridge][par_bridge]
  [commit](https://github.com/jakub-m/llama2-rs/commit/f4d9041). Slapping
  naively `par_bridge` is slower that sequential execution on a single code. I
  suppose it's the overhead of coordination of those small work chunks.

- 132 tok/s - with naive use of rayon and [par_iter][par_iter].
  [commit](https://github.com/jakub-m/llama2-rs/commit/8eda5d5). Rayon is
  awesome!

We can further tweak Rayon to take larger chunks of work, to spend less on
coordination. Here are the results of modifying [`with_min_len`][with_min_len]
parameter:

- 1 - 132 tok/s
- 5 - 146 tok/s
- 10 - 153 tok/s
- 15 - 154 tok/s - best `with_min_len` value. [commit](https://github.com/jakub-m/llama2-rs/commit/b596ff4) 
- 20 - 150 tok/s
- 40 - 142 tok/s
- 70 - 115 tok/s 
- 150 - 93 tok/s

FTR, the machine I use has [available parallelism][available_parallelism] is 12
(8 performance and 4 efficiency cores).

[par_bridge]: https://docs.rs/rayon/latest/rayon/iter/trait.ParallelBridge.html
[par_iter]: https://docs.rs/rayon/1.11.0/rayon/iter/index.html
[with_min_len]: https://docs.rs/rayon/1.11.0/rayon/iter/trait.IndexedParallelIterator.html#method.with_min_len
[available_parallelism]: https://doc.rust-lang.org/std/thread/fn.available_parallelism.html


# Metal

I use Apple Metal to interface GPU and run matrix multiplciation on GPU. Examples:

- [metal_add.rs](examples/metal_add.rs) implements a simple addition in GPU using a shared memory buffer.
- [metal_matmul.rs](examples/metal_matmul.rs) runs matrix multiplication on GPU.

Caveat: for TinyStories the benchamrks are in tok/s (higher better) and for
llama it's s/tok (lower bertter).

Running llama2 (`make run-napalm`) with matmul naively computed in GPU (shared
memory buffers, Metal native matmul) yields ~20% GPU utilization, and ~60
s/tok. For CPU with Rayon, it's ~20 sec per token. So, just throwing naively
GPU at the problem didn't help at all.

Initially all the models were in f32, I later cast the models to f16 (1/2 mem
usage). Surprisingly, using f16 made TinyStories inference _slower_ (12.8 tok/s
with f16, drop from 16 tok/s with f32).

I played with shared GPU memory and private GPU memory. In theory private mem
should work faster since there there is no cache synchronization with CPU. But,
when I moved the weights from shared mem to private mem, the inference was
somewhat slower: I got 4.26 ms per matrix multiplication with shared buffer,
and 4.52 ms for private buffer -
[commit](https://github.com/jakub-m/llama2-rs/commit/1c16e28). Maybe it was
because matrix W was in private mem and vector X was in shared mem?

I optimised the code to not wait for GPU result when I don't need to
(pipelining). That is, I push commands to Metal's command buffer, those
commands run in parallel to CPU, and I wait (blocking) for the result when I
need it. With the pipelined execution on GPU I got 18.5 tok/s on TinyStories
and 3.5 s/tok on llama2. Note that for TinyStories, it was way faster to run it
with Rayon and CPUs. For llama2, it's faster to run it on GPU.


# Materials

- [From Multi-Head to Latent Attention: The Evolution of Attention Mechanisms](https://vinithavn.medium.com/from-multi-head-to-latent-attention-the-evolution-of-attention-mechanisms-64e3c0505f24)
- [Positional Embeddings in Transformers: A Math Guide to RoPE & ALiBi](https://towardsdatascience.com/positional-embeddings-in-transformers-a-math-guide-to-rope-alibi/)
- [LLM Embeddings Explained: A Visual and Intuitive Guide](https://huggingface.co/spaces/hesamation/primer-llm-embedding)
- [SwiGLU](https://medium.com/@s_boudefel/exploring-swiglu-the-activation-function-powering-modern-llms-9697f88221e7)
- Apple Metal:
  - [Setting up a command structure](https://developer.apple.com/documentation/metal/setting-up-a-command-structure) 
  - [Choosing a resource storage mode for Apple GPUs](https://developer.apple.com/documentation/metal/choosing-a-resource-storage-mode-for-apple-gpus)
  - [Resource Options](https://developer.apple.com/library/archive/documentation/3DDrawing/Conceptual/MTLBestPracticesGuide/ResourceOptions.html)


# Where to go from here

Some next ideas to further improve the code

- Support llama3
- Use LLM.int8 quantization. Here [post about emergent features][llm_emergent] and this [post on HuggingFace][llm_int8_hf]

[llm_emergent]: https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/
[llm_int8_hf]: https://huggingface.co/blog/hf-bitsandbytes-integration]

